{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n",
    "<!-- <h3>Base Model</h3> -->\n",
    "- Epsilon = 1\n",
    "- Epsilon_min = 0.01\n",
    "- Epsilon_decay = 0.99\n",
    "- Discount_rate = 0.8\n",
    "- Train_start = 1000\n",
    "- Batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self,InputShape=4, \n",
    "                 Epsilon = 0.1, \n",
    "                 Epsilon_Decay=0.98,\n",
    "                 NActions=2,\n",
    "                 UpdatePerEp=1,\n",
    "                 Dense=64,\n",
    "                 BatchSize=32,\n",
    "                 learning_rate=0.01,\n",
    "                 weights = None):\n",
    "        \n",
    "        # Preset Parameters. #\n",
    "        self.Epsilon = Epsilon\n",
    "        self.InputShape = InputShape\n",
    "        self.ReplayMemorySize = 10000\n",
    "        self.MinReplayMemory = 1000\n",
    "\n",
    "        # Hyperparameters. # (Tuneable)\n",
    "        self.NActions = NActions\n",
    "        self.NoUpdate = UpdatePerEp\n",
    "        self.Dense = Dense\n",
    "        self.BatchSize = BatchSize\n",
    "        self.Memory = deque(maxlen=self.ReplayMemorySize)\n",
    "        self.Epsilon_Decay = Epsilon_Decay\n",
    "        \n",
    "        # Build Target And Main Model\n",
    "\n",
    "        self.Main = self.CreateModel(\"Main\")\n",
    "        self.Target = self.CreateModel(\"Target\")\n",
    "\n",
    "        # Load Weights if there are\n",
    "        if weights != None:\n",
    "            self.Main.load_weights(weights)\n",
    "        self.Target.set_weights(self.Main.get_weights())\n",
    "        self.Optimiser = Adam()\n",
    "\n",
    "        # Target network update counter. #\n",
    "        self.TargetUpdateCounter = 0\n",
    "\n",
    "    def CreateModel(self, Type):\n",
    "        inputs = Input(shape = (self.InputShape), name = 'Input')\n",
    "        x = Dense(self.Dense, activation = 'relu', name = '1stHiddenLayer')(inputs)\n",
    "        x = Dense(self.Dense, activation = 'relu', name = '2ndHiddenLayer')(x)\n",
    "        outputs = Dense(self.NActions, activation = 'linear', name = 'Output')(x)\n",
    "        \n",
    "        NN = Model(inputs, outputs, name = f'{Type}')\n",
    "        NN.summary()\n",
    "\n",
    "        return NN\n",
    "\n",
    "    def UpdateMemory(self, info):\n",
    "        self.Memory.append(info)\n",
    "\n",
    "    def pls_save_weights(self, name):\n",
    "        self.Main.save_weights(f\"DQN_weights/Main{name}.h5\")\n",
    "        self.Target.save_weights(f\"DQN_weights/Target{name}.h5\")\n",
    "\n",
    "    def PendulumActionConverter(self, A):\n",
    "        ActualTorque = (A / self.NActions - 0.5) * 4\n",
    "        return ActualTorque\n",
    "\n",
    "    def PendulumInverseActionConverter(self, A):\n",
    "        ActualA = round((A + 2) * (self.NActions - 1) / 4)\n",
    "        return  (ActualA)\n",
    "\n",
    "    def Get_Action(self, state, env):\n",
    "        if np.random.rand() < self.Epsilon:\n",
    "            action = env.action_space.sample()\n",
    "            a = self.PendulumInverseActionConverter(action[0])\n",
    "            return action, a\n",
    "        else:\n",
    "            step = np.argmax(state)\n",
    "            a = self.PendulumActionConverter(step)\n",
    "            action = np.array([a])\n",
    "            a = self.PendulumInverseActionConverter(a)\n",
    "            return action, a\n",
    "\n",
    "    def decay(self):\n",
    "        self.Epsilon = self.Epsilon*self.Epsilon_Decay\n",
    "    \n",
    "    def Train(self, EndOfEpisode):\n",
    "        if len(self.Memory) < self.MinReplayMemory:\n",
    "            return\n",
    "        \n",
    "        TrainingData = random.sample(self.Memory, self.BatchSize)\n",
    "\n",
    "        ListOfS = np.array([element[0] for element in TrainingData])\n",
    "\n",
    "        ListOfQ = np.array(self.Main(ListOfS))\n",
    "\n",
    "        ListOfSNext = np.array([element[3] for element in TrainingData])\n",
    "        ListOfQNext = self.Target(ListOfSNext)\n",
    "\n",
    "        X = []\n",
    "        Y = []\n",
    "        for index, (S, A, R, SNext, Done) in enumerate(TrainingData):\n",
    "            if not Done:\n",
    "                MaxQNext = np.max(ListOfQNext[index])\n",
    "                QNext = R + self.Epsilon * MaxQNext\n",
    "            else:\n",
    "                QNext = R\n",
    "            Q = ListOfQ[index]\n",
    "            Q[A] = QNext\n",
    "\n",
    "            X.append(S)\n",
    "            Y.append(Q)\n",
    "\n",
    "        self.GTfit(X, Y)\n",
    "\n",
    "        # Update target network every episode. #\n",
    "        if EndOfEpisode:\n",
    "            self.TargetUpdateCounter += 1\n",
    "\n",
    "        if self.TargetUpdateCounter >= self.NoUpdate:\n",
    "            self.Target.set_weights(self.Main.get_weights())\n",
    "            self.TargetUpdateCounter = 0\n",
    "\n",
    "    @tf.function\n",
    "    def GTfit(self, X, Y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            Predictions = self.Main(tf.convert_to_tensor(X), training=True)\n",
    "            Loss = tf.math.reduce_mean(\n",
    "                tf.math.square(tf.convert_to_tensor(Y) - Predictions)\n",
    "            )\n",
    "        Grad = tape.gradient(Loss, self.Main.trainable_variables)\n",
    "        self.Optimiser.apply_gradients(zip(Grad, self.Main.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnvName = 'Pendulum-v0'\n",
    "InputShape = 3\n",
    "def Main(episodes = 300, \n",
    "         filename = \"base\", \n",
    "         InputShape=3, \n",
    "         NActions=40, \n",
    "         Dense=64, \n",
    "         Epsilon=0.1, \n",
    "         Epsilon_Decay=0.98, \n",
    "         UpdatePerEp=3):\n",
    "    # Instantiate DQN\n",
    "    dqn = DQN(InputShape=InputShape, \n",
    "              NActions=NActions, \n",
    "              Dense=Dense, \n",
    "              Epsilon=Epsilon, \n",
    "              Epsilon_Decay=Epsilon_Decay, \n",
    "              UpdatePerEp=UpdatePerEp)\n",
    "    MovingAverage = []\n",
    "    ListOfScores = []\n",
    "    ShowEvery = 25\n",
    "    # Store Scores and Moving Average\n",
    "    score = 0\n",
    "    for i in range(episodes):\n",
    "        print(\"episode\", i)\n",
    "        Done = False\n",
    "        counter = 0\n",
    "        env = gym.make(f'{EnvName}')\n",
    "        S = env.reset()\n",
    "        score = 0  # Initialize score for each episode\n",
    "\n",
    "        while not Done: \n",
    "            Q = dqn.Main(S.reshape(-1, S.shape[0]))\n",
    "            action, A = dqn.Get_Action(Q, env)\n",
    "\n",
    "            if not i % ShowEvery and len(dqn.Memory) >= dqn.MinReplayMemory:\n",
    "                env.render()\n",
    "\n",
    "            SNext, R, Done, Info = env.step(action)\n",
    "            \n",
    "            dqn.UpdateMemory((S, A, R, SNext, Done))\n",
    "            dqn.Train(Done)\n",
    "            score += R\n",
    "            S = SNext\n",
    "            if Done:\n",
    "                # dqn.decay()\n",
    "                ListOfScores.append(score)\n",
    "                if score >= max(ListOfScores):\n",
    "                    dqn.pls_save_weights(filename)\n",
    "                MovingAverage.append(np.mean(ListOfScores[-100:]))\n",
    "                print(f'Finished! Return: {score}', print(f'DQN Mem: {len(dqn.Memory)} DQN Ep : {dqn.Epsilon}'))\n",
    "                env.close()\n",
    "    return MovingAverage, ListOfScores\n",
    "episode_number = np.arange(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 300\n",
    "episode_number = np.arange(EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base_MA_Scores, Base_Scores = Main(episodes=EPISODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.lineplot(x=episode_number, y=Base_Scores, label=\"Score\")\n",
    "plt.title('Average Reward Per Episode')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.lineplot(x=episode_number, y=Base_MA_Scores, label=\"Average Score\")\n",
    "plt.title('Moving Average Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_actions = [30,40,50]\n",
    "Dense_tune = [40, 64, 88]\n",
    "Epsilon = [0.1, 0.2, 0.3]\n",
    "Epsilon_Decay = [0.98, 0.97, 0.96, 0.95]\n",
    "UpdateNo = [1,2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Actions/Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAction_average_score = [] # Array of Array of Moving Average Scores for each Bins & Episode\n",
    "NAaction_Scores = [] # Array of Array of Scores for each Bins & Episode\n",
    "\n",
    "for e in number_of_actions:\n",
    "    bins_scores_ma, bins_res_scores, = \\\n",
    "        Main(NActions = e, filename=f\"NAction{e}\")\n",
    "    NAction_average_score.append(bins_scores_ma)\n",
    "    NAaction_Scores.append(bins_res_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Average Score\n",
    "plt.subplot(2,1,1)\n",
    "for i in range(len(number_of_actions)):\n",
    "    sns.lineplot(x=episode_number, y=NAaction_Scores[i], label=number_of_actions[i])\n",
    "plt.title('Score')\n",
    "\n",
    "# Moving Average Score\n",
    "plt.subplot(2, 1, 2)\n",
    "for i in range (len(number_of_actions)):\n",
    "    sns.lineplot(x=episode_number, y=NAction_average_score[i], label=number_of_actions[i])\n",
    "plt.title('Moving Average Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dense_average_score = [] # Array of Array of Moving Average Scores for each Bins & Episode\n",
    "Dense_Scores = [] # Array of Array of Scores for each Bins & Episode\n",
    "\n",
    "for e in Dense_tune:\n",
    "    Dense_scores_ma, Dense_res_scores, = \\\n",
    "        Main(NAaction = 40, Dense=e, filename=f\"Dense{e}\")\n",
    "    Dense_average_score.append(Dense_scores_ma)\n",
    "    Dense_Scores.append(Dense_res_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Average Score\n",
    "plt.subplot(2,1,1)\n",
    "for i in range(len(Dense_tune)):\n",
    "    sns.lineplot(x=episode_number, y=Dense_average_score[i], label=Dense_tune[i])\n",
    "plt.title('Score')\n",
    "\n",
    "# Moving Average Score\n",
    "plt.subplot(2, 1, 2)\n",
    "for i in range (len(Dense_tune)):\n",
    "    sns.lineplot(x=episode_number, y=Dense_Scores[i], label=Dense_tune[i])\n",
    "plt.title('Moving Average Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UpdateNo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
