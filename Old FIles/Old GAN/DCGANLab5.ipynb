{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1166,"status":"ok","timestamp":1610444248563,"user":{"displayName":"Dr Peter Leong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLUXmhYkYUye0bBVbi1z-CrtAoHoERE3zkmKf7=s64","userId":"00561202458267680023"},"user_tz":-480},"id":"q-t1gx9zONV8"},"outputs":[],"source":["from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout\n","from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.layers import UpSampling2D, Conv2D\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.optimizers import Adam\n","\n","import matplotlib.pyplot as plt\n","import sys\n","import numpy as np\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":2038,"status":"ok","timestamp":1610444249440,"user":{"displayName":"Dr Peter Leong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLUXmhYkYUye0bBVbi1z-CrtAoHoERE3zkmKf7=s64","userId":"00561202458267680023"},"user_tz":-480},"id":"0lWLj3rhONWC"},"outputs":[],"source":["class DCGAN():\n","    def __init__(self, rows, cols, channels, z = 10):\n","        # Input shape\n","        self.img_rows = rows\n","        self.img_cols = cols\n","        self.channels = channels\n","        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","        self.latent_dim = z\n","\n","        optimizer = Adam(0.0001, 0.5)\n","\n","        # Build and compile the discriminator\n","        self.discriminator = self.build_discriminator()\n","        self.discriminator.compile(loss='binary_crossentropy',\n","            optimizer=optimizer,\n","            metrics=['accuracy'])\n","\n","        # Build the generator\n","        self.generator = self.build_generator()\n","\n","        # The generator takes noise as input and generates imgs\n","        z = Input(shape=(self.latent_dim,))\n","        img = self.generator(z)\n","\n","        # For the combined model we will only train the generator\n","        self.discriminator.trainable = False\n","\n","        # The discriminator takes generated images as input and determines validity\n","        valid = self.discriminator(img)\n","\n","        # The combined model  (stacked generator and discriminator)\n","        # Trains the generator to fool the discriminator\n","        self.combined = Model(z, valid)\n","        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n","\n","    def build_generator(self):\n","\n","        model = Sequential()\n","\n","        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n","        model.add(Reshape((7, 7, 128)))\n","        model.add(UpSampling2D())\n","        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Activation(\"relu\"))\n","        model.add(UpSampling2D())\n","        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(Activation(\"relu\"))\n","        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n","        model.add(Activation(\"tanh\"))\n","\n","        model.summary()\n","\n","        noise = Input(shape=(self.latent_dim,))\n","        img = model(noise)\n","\n","        return Model(noise, img)\n","\n","    def build_discriminator(self):\n","\n","        model = Sequential()\n","\n","        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.25))\n","        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n","        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.25))\n","        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.25))\n","        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n","        model.add(BatchNormalization(momentum=0.8))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.25))\n","        model.add(Flatten())\n","        model.add(Dense(1, activation='sigmoid'))\n","\n","        model.summary()\n","\n","        img = Input(shape=self.img_shape)\n","        validity = model(img)\n","\n","        return Model(img, validity)\n","\n","    def train(self, epochs, batch_size=256, save_interval=50):\n","\n","        # Load the dataset\n","        (X_train, _), (_, _) = mnist.load_data()\n","\n","        # Rescale -1 to 1\n","        X_train = X_train / 127.5 - 1.\n","        X_train = np.expand_dims(X_train, axis=3)\n","\n","        # Adversarial ground truths\n","        valid = np.ones((batch_size, 1))\n","        fake = np.zeros((batch_size, 1))\n","\n","        for epoch in range(epochs):\n","\n","            # ---------------------\n","            #  Train Discriminator\n","            # ---------------------\n","            if epoch % 2 == 0:\n","                # Select a random half of images\n","                idx = np.random.randint(0, X_train.shape[0], batch_size)\n","                imgs = X_train[idx]\n","\n","                # Sample noise and generate a batch of new images\n","                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n","                gen_imgs = self.generator.predict(noise)\n","\n","                # Train the discriminator (real classified as ones and generated as zeros)\n","                d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n","                d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n","                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","\n","            # ---------------------\n","            #  Train Generator\n","            # ---------------------\n","\n","            # Train the generator (wants discriminator to mistake images as real)\n","            g_loss = self.combined.train_on_batch(noise, valid)\n","\n","            # Plot the progress\n","            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n","\n","            # If at save interval => save generated image samples\n","            if epoch % save_interval == 0:\n","                self.save_imgs(epoch)\n","\n","    def save_imgs(self, epoch):\n","        r, c = 5, 5\n","        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n","        gen_imgs = self.generator.predict(noise)\n","\n","        # Rescale images 0 - 1\n","        gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","        fig, axs = plt.subplots(r, c)\n","        cnt = 0\n","        for i in range(r):\n","            for j in range(c):\n","                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n","                axs[i,j].axis('off')\n","                cnt += 1\n","        fig.savefig(\"dcgan_mnist_{:d}.png\".format(epoch))\n","        plt.close()\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":225106,"status":"ok","timestamp":1610444472512,"user":{"displayName":"Dr Peter Leong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLUXmhYkYUye0bBVbi1z-CrtAoHoERE3zkmKf7=s64","userId":"00561202458267680023"},"user_tz":-480},"id":"qn_Sz2vvPRJz","outputId":"ddd1b6d2-9dcd-4a1f-f8f2-62549ae85608"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_14 (Conv2D)          (None, 14, 14, 32)        320       \n","                                                                 \n"," leaky_re_lu_8 (LeakyReLU)   (None, 14, 14, 32)        0         \n","                                                                 \n"," dropout_8 (Dropout)         (None, 14, 14, 32)        0         \n","                                                                 \n"," conv2d_15 (Conv2D)          (None, 7, 7, 64)          18496     \n","                                                                 \n"," zero_padding2d_2 (ZeroPaddi  (None, 8, 8, 64)         0         \n"," ng2D)                                                           \n","                                                                 \n"," batch_normalization_10 (Bat  (None, 8, 8, 64)         256       \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_9 (LeakyReLU)   (None, 8, 8, 64)          0         \n","                                                                 \n"," dropout_9 (Dropout)         (None, 8, 8, 64)          0         \n","                                                                 \n"," conv2d_16 (Conv2D)          (None, 4, 4, 128)         73856     \n","                                                                 \n"," batch_normalization_11 (Bat  (None, 4, 4, 128)        512       \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_10 (LeakyReLU)  (None, 4, 4, 128)         0         \n","                                                                 \n"," dropout_10 (Dropout)        (None, 4, 4, 128)         0         \n","                                                                 \n"," conv2d_17 (Conv2D)          (None, 4, 4, 256)         295168    \n","                                                                 \n"," batch_normalization_12 (Bat  (None, 4, 4, 256)        1024      \n"," chNormalization)                                                \n","                                                                 \n"," leaky_re_lu_11 (LeakyReLU)  (None, 4, 4, 256)         0         \n","                                                                 \n"," dropout_11 (Dropout)        (None, 4, 4, 256)         0         \n","                                                                 \n"," flatten_2 (Flatten)         (None, 4096)              0         \n","                                                                 \n"," dense_4 (Dense)             (None, 1)                 4097      \n","                                                                 \n","=================================================================\n","Total params: 393,729\n","Trainable params: 392,833\n","Non-trainable params: 896\n","_________________________________________________________________\n","Model: \"sequential_5\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_5 (Dense)             (None, 6272)              68992     \n","                                                                 \n"," reshape_2 (Reshape)         (None, 7, 7, 128)         0         \n","                                                                 \n"," up_sampling2d_4 (UpSampling  (None, 14, 14, 128)      0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_18 (Conv2D)          (None, 14, 14, 128)       147584    \n","                                                                 \n"," batch_normalization_13 (Bat  (None, 14, 14, 128)      512       \n"," chNormalization)                                                \n","                                                                 \n"," activation_6 (Activation)   (None, 14, 14, 128)       0         \n","                                                                 \n"," up_sampling2d_5 (UpSampling  (None, 28, 28, 128)      0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_19 (Conv2D)          (None, 28, 28, 64)        73792     \n","                                                                 \n"," batch_normalization_14 (Bat  (None, 28, 28, 64)       256       \n"," chNormalization)                                                \n","                                                                 \n"," activation_7 (Activation)   (None, 28, 28, 64)        0         \n","                                                                 \n"," conv2d_20 (Conv2D)          (None, 28, 28, 1)         577       \n","                                                                 \n"," activation_8 (Activation)   (None, 28, 28, 1)         0         \n","                                                                 \n","=================================================================\n","Total params: 291,713\n","Trainable params: 291,329\n","Non-trainable params: 384\n","_________________________________________________________________\n","8/8 [==============================] - 0s 50ms/step\n","0 [D loss: 1.127907, acc.: 32.03%] [G loss: 0.707205]\n","1/1 [==============================] - 0s 76ms/step\n","1 [D loss: 1.127907, acc.: 32.03%] [G loss: 0.700969]\n","8/8 [==============================] - 0s 42ms/step\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dcgan \u001b[38;5;241m=\u001b[39m DCGAN(\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdcgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[1;32mIn[7], line 127\u001b[0m, in \u001b[0;36mDCGAN.train\u001b[1;34m(self, epochs, batch_size, save_interval)\u001b[0m\n\u001b[0;32m    120\u001b[0m     d_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39madd(d_loss_real, d_loss_fake)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# ---------------------\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m#  Train Generator\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# ---------------------\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Train the generator (wants discriminator to mistake images as real)\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombined\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Plot the progress\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m [D loss: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m, acc.: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m] [G loss: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch, d_loss[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39md_loss[\u001b[38;5;241m1\u001b[39m], g_loss))\n","File \u001b[1;32mc:\\Users\\p2214478\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\engine\\training.py:2383\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2380\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_train_function()\n\u001b[0;32m   2381\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m-> 2383\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2384\u001b[0m \u001b[39mif\u001b[39;00m return_dict:\n\u001b[0;32m   2385\u001b[0m     \u001b[39mreturn\u001b[39;00m logs\n","File \u001b[1;32mc:\\Users\\p2214478\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[0;32m    633\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[1;32m--> 635\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n","File \u001b[1;32mc:\\Users\\p2214478\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n","File \u001b[1;32mc:\\Users\\p2214478\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n","File \u001b[1;32mc:\\Users\\p2214478\\.conda\\envs\\gpu_env\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    626\u001b[0m     \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 628\u001b[0m         t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m    629\u001b[0m     \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[39m# as-is.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n","File \u001b[1;32mc:\\Users\\p2214478\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n","File \u001b[1;32mc:\\Users\\p2214478\\.conda\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[0;32m   1124\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["dcgan = DCGAN(28,28,1)\n","dcgan.train(epochs=5000, batch_size=256, save_interval=50)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"DCGANLab5.ipynb","provenance":[]},"kernelspec":{"display_name":"gpu_env","language":"python","name":"gpu_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"vscode":{"interpreter":{"hash":"911ffd3c74f64fab979f4bdb0993aae78a6274f1459e312fc9fbcec5c1b8e42f"}}},"nbformat":4,"nbformat_minor":0}
